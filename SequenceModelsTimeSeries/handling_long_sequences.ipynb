{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Fighting the unstable gradients problem\n",
    "Many of the tricks we used in DNN, can be also used for RNN's: **good parameter initialization**, **faster optimizers**, **dropout** .....\n",
    "However, Relu's are not used very often in RNN's, because they can make the RNN more unstable due to the **exploding gradients problem**.\n",
    "so, instead of Relu's, we use the **hyperbolic tangent activation function (tanh)** which is similar to the sigmoid function but outputs values between -1 and 1, so the mean of its output is much closer to zero, and so this helps reduce the exploding gradients problem.\n",
    "\n",
    "Moreover, BatchNormalization doesnt work well with RNN's, because it is applied at each time step for inputs and hidden state, which will slow down training significantly. However, it is possible to use it between RNN layers, but it is not very common and slow down training also.\n",
    "\n",
    "Another form of normalization is **Layer Normalization** which is similar to Batch Normalization, but instead of normalizing across the batch dimension, it normalizes across the features dimension."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ea815be06a12950"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "class LNSimpleRNNCell(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, activation=\"tanh\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.state_size = units\n",
    "        self.output_size = units\n",
    "        self.simple_rnn_cell = tf.keras.layers.SimpleRNNCell(units, activation=None)\n",
    "        \n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "    \n",
    "    def call(self, inputs, states):\n",
    "        outputs, new_states = self.simple_rnn_cell(inputs, states)\n",
    "        norm_outputs = self.activation(self.layer_norm(outputs))\n",
    "        \n",
    "        return norm_outputs, [norm_outputs]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-18T19:31:39.879132200Z",
     "start_time": "2023-11-18T19:31:32.457529400Z"
    }
   },
   "id": "aa5a91870ccb9882"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
